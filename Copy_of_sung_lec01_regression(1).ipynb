{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of sung_lec01_regression(1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sturu1/THU-PM/blob/master/Copy_of_sung_lec01_regression(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y8UEW4k5S8h",
        "colab_type": "text"
      },
      "source": [
        "## Simple Linear Regression 구현 실습(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xgpg79p5Y-e",
        "colab_type": "text"
      },
      "source": [
        "> 먼저 필요한 Keras 및 numpy 라이브러리를 불러옵니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KJ5XU2sCQUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.optimizers import SGD\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0GjCd6E5g1n",
        "colab_type": "text"
      },
      "source": [
        "> 실습에 사용할 데이터를 numpy 배열로 생성합니다. \n",
        "총 4개의 입력 및 출력 샘플을 정의합니다. \n",
        "\n",
        "> 입력데이터는 4개의 샘플(개체)로 이루어져 있으며, 각 개체는 한개의 속성을 가지고 있습니다. 출력데이터도 4개의 샘플, 1개의 속성으로 이루어져 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8wzURqERMCe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = np.array([[1], [2], [3], [4]])\n",
        "y_train = np.array([[0], [-1], [-2], [-3]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOlL0J07yzHL",
        "colab_type": "text"
      },
      "source": [
        "> 이러한 입력, 출력데이터간의 관계를 가장 잘 설명하는 규칙을 찾는 것이 문제입니다. simple linear regression에서는 입력과 출력 데이터 간의 관계를 선형으로 가정합니다. 즉, 다음의 방정식을 만족시키는 w1과 b 값을 찾는 것이 학습이라고 할 수 있습니다. \n",
        "``` \n",
        "y = w1*x1 + b\n",
        "```\n",
        "프로그래밍을 통해 학습하기 전에, 먼저 여러분들이 예측해 보세요. w1과 b 값은 얼마이겠습니까? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ul3HfVkr5rhA",
        "colab_type": "text"
      },
      "source": [
        "각 입력 및 출력 데이터의 차원과 shape를 확인합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYdFmYKXSZAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "83409308-152b-4b98-adb9-81798fd40c72"
      },
      "source": [
        "print(x_train.ndim)\n",
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "(4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lt8VQOes5zv_",
        "colab_type": "text"
      },
      "source": [
        "> 이제 Keras를 이용하여 **w1**과**b** 값을 찾는 학습을 진행해 보겠습니다. 먼저 여러분은 **모델을 생성**해야 합니다. 모델을 생성한다는 것은 마치 학습을 위한 뇌의 구조를 만드는 작업에 비유할 수 있습니다. 즉, 네트워크의 구조(뇌의 뉴런의 구조), 학습을 위해 필요한 손실함수 및 optimzer 설정(Gradient Descent Algorithm)등을 정의하는 것입니다. \n",
        "\n",
        "> 복잡한 모델을 만들면 보다 복잡한 문제를 학습할 수 있지만 학습에 오랜 시간이 걸리고, 단순한 모델을 만들면 단순한 문제만 학습할 수 있지만 학습에 걸리는 시간이 짧아집니다. 마치, 특정 문제를 해결하는데 인간의 뇌를 사용할지, 강아지의 뇌를 사용할지, 곤충의 뇌를 사용할지를 정하는 것에 비유할 수 있습니다. \n",
        "\n",
        "> 이번 예제에서는 **Keras의 Sequential() 함수**을을 이용하여 네트워크의 구조를 정의합니다. 이 예제에서는 1개의 layer(층)과 하나의 셀만을 가진 매우 단순한 구조로 network를 정의하였습니다. 사실 신경망이라고 볼수도 없습니다. \n",
        "\n",
        "> 본 예제는 regression 기법을 구현하는 것으로, activation 함수(활성 함수)로 linear를 선택하였습니다. 이는 활성함수 입력 전 후로 아무런 변화가 없음을 의미합니다. 결론적으로 Dense 층이 의미하는 것은 다음과 같은 수식입니다. \n",
        "```\n",
        "y=w1 * x1 + b\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esWivA_GRN8m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoY7yXfq2COr",
        "colab_type": "text"
      },
      "source": [
        "> 이제 모델에 layer(층)와 unit(셀, 뉴런)을 순차적으로 추가합니다. 이때, 입력데이터의 각 개체의 모양을 input_shape로 정의하는 것이 필요합니다. input 데이터는 4개의 개체(4명의 학생)로 이루어져 있지만, 한 개체는 1개의 속성을 가지고 있으므로 **inpu_shape는 1D 벡터인 (3,)**로 지정합니다. 또는 input_dim=3 으로 표현하기도 합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0925d-tV5QdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(Dense(1, input_shape = (1,), activation='linear'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICjqYsjFTPxA",
        "colab_type": "text"
      },
      "source": [
        "> 참고로 윗 식은 다음과 같이 표현 가능합니다.\n",
        "```\n",
        "model.add(Dense(1, input_dim=1))\n",
        "```\n",
        "(P70 참조) Dense 함수에서 activation이 linear일 경우, 그 역할은 다음처럼 단순 선형 변환입니다. \n",
        "```\n",
        "outputput = dot(W,input)+b\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0DKMSUslVtKN",
        "colab_type": "text"
      },
      "source": [
        "> 다음으로 모델을 컴파일 합니다. 이때 활용할 손실함수(loss function or cost function) 및 Gradient Descent 알고리즘을 정의합니다. \n",
        "mse는 평균제곱오차를 손실함수로 사용한다는 의미이고,\n",
        "SGD는 Gradient Descent Algorithm 중 하나로 확률적경사하강법을 의미합니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liQW9s2MRRj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss='mse', optimizer=SGD(lr=0.1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOQu510eVSud",
        "colab_type": "text"
      },
      "source": [
        "> 자 이제 모델의 생성이 끝났습니다. \n",
        "최종 학습 이전에 모델의 구조를 확인해 볼까요? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tDAAaAeVSLm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "c0b1fa88-63ea-4a2b-d919-456b6412fddd"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 1)                 2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xNMeDr2VnV9",
        "colab_type": "text"
      },
      "source": [
        "> 이제 입력 및 출력 샘플을 가지고 모델을 학습 시킵니다. \n",
        "> 한번에 학습되지 않겠지요? 총 200번 반복하면서 점점 학습의 정확도를 높입니다. 여기서 모든 입력데이터를 전부 사용하여 학습하는 것을 **1 epoch** 학습하였다고 표현합니다. 우리는 200번을 학습시킬 계획이므로 **epochs=200**으로 설정합니다 ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fO55DiksVlWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(x_train, y_train, epochs=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndRDDAi33m26",
        "colab_type": "text"
      },
      "source": [
        "> 학습이 진행되는 동안 학습 결과와 실제 출력값의 차리를 나타내는 loss 값이 점차 줄고 있는 것을 확인할 수 있습니다. 학습이 잘 진행되고 있다는 의미겠지요. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR2GyR2FWrNu",
        "colab_type": "text"
      },
      "source": [
        "> 그럼, 모델의 학습결과, 즉 모델의 가중치를 확인해 볼까요? \n",
        "w값은 거의 -1, b 값은 거의 1이 나오는 것을 알 수 있지요? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh2ieLwMWQI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a701fcd0-f1d7-4a0a-d8bc-853f88f7c9a9"
      },
      "source": [
        "model.weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tf.Variable 'dense_4/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-0.9986859]], dtype=float32)>,\n",
              " <tf.Variable 'dense_4/bias:0' shape=(1,) dtype=float32, numpy=array([0.99613637], dtype=float32)>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXD5XdLi36wg",
        "colab_type": "text"
      },
      "source": [
        "> 학습이 진행되는 동안(epoch) 손실값(실제값과 학습값과의 차이)의 변화를 확인해 보겠습니다. 그래프를 그리는 도구인 matplotlib 모듈을 사용하겠습니다. \n",
        "\n",
        "> 학습이 진행되는 동안 손실값은 history.history라는 변수에 dictionary 형태로 저장되어 있습니다. 따라서 여러분은 다음과 같이 100번의 반복동안 손실함수의 값(mse)을 확인할 수 있습니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jy9JN_M4eCS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52f063e4-8204-46b7-adbd-03fffd640c88"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'loss': [38.517608642578125,\n",
              "  17.52765464782715,\n",
              "  8.089747428894043,\n",
              "  3.83931827545166,\n",
              "  1.918734073638916,\n",
              "  1.0449285507202148,\n",
              "  0.6417853236198425,\n",
              "  0.4505993127822876,\n",
              "  0.35517868399620056,\n",
              "  0.3033202290534973,\n",
              "  0.27155202627182007,\n",
              "  0.24930456280708313,\n",
              "  0.23180268704891205,\n",
              "  0.21687448024749756,\n",
              "  0.20351892709732056,\n",
              "  0.19126182794570923,\n",
              "  0.17986717820167542,\n",
              "  0.16920727491378784,\n",
              "  0.15920424461364746,\n",
              "  0.14980380237102509,\n",
              "  0.14096347987651825,\n",
              "  0.13264717161655426,\n",
              "  0.1248224750161171,\n",
              "  0.11745982617139816,\n",
              "  0.11053163558244705,\n",
              "  0.10401222109794617,\n",
              "  0.09787734597921371,\n",
              "  0.09210438281297684,\n",
              "  0.08667190372943878,\n",
              "  0.08155983686447144,\n",
              "  0.0767492800951004,\n",
              "  0.07222248613834381,\n",
              "  0.06796266883611679,\n",
              "  0.06395412981510162,\n",
              "  0.06018199026584625,\n",
              "  0.056632377207279205,\n",
              "  0.05329208821058273,\n",
              "  0.050148822367191315,\n",
              "  0.04719095677137375,\n",
              "  0.04440757632255554,\n",
              "  0.04178832843899727,\n",
              "  0.03932357579469681,\n",
              "  0.03700420260429382,\n",
              "  0.034821633249521255,\n",
              "  0.03276779130101204,\n",
              "  0.030835090205073357,\n",
              "  0.029016394168138504,\n",
              "  0.02730494551360607,\n",
              "  0.02569444477558136,\n",
              "  0.02417895942926407,\n",
              "  0.022752832621335983,\n",
              "  0.02141083963215351,\n",
              "  0.02014799416065216,\n",
              "  0.018959620967507362,\n",
              "  0.01784135028719902,\n",
              "  0.016789043322205544,\n",
              "  0.015798799693584442,\n",
              "  0.014866947196424007,\n",
              "  0.01399006973952055,\n",
              "  0.013164911419153214,\n",
              "  0.01238842774182558,\n",
              "  0.011657742783427238,\n",
              "  0.010970141738653183,\n",
              "  0.010323099792003632,\n",
              "  0.009714232757687569,\n",
              "  0.009141271002590656,\n",
              "  0.008602100424468517,\n",
              "  0.00809473730623722,\n",
              "  0.007617291994392872,\n",
              "  0.007168016862124205,\n",
              "  0.006745230406522751,\n",
              "  0.006347385235130787,\n",
              "  0.005973010789602995,\n",
              "  0.00562070868909359,\n",
              "  0.005289193242788315,\n",
              "  0.004977232310920954,\n",
              "  0.004683664068579674,\n",
              "  0.0044074165634810925,\n",
              "  0.004147454630583525,\n",
              "  0.0039028350729495287,\n",
              "  0.0036726328544318676,\n",
              "  0.0034560179337859154,\n",
              "  0.0032521802932024,\n",
              "  0.003060361836105585,\n",
              "  0.002879851032048464,\n",
              "  0.0027099978178739548,\n",
              "  0.0025501586496829987,\n",
              "  0.002399745862931013,\n",
              "  0.0022582008969038725,\n",
              "  0.0021250115241855383,\n",
              "  0.001999670173972845,\n",
              "  0.0018817288801074028,\n",
              "  0.0017707438673824072,\n",
              "  0.0016662998823449016,\n",
              "  0.0015680212527513504,\n",
              "  0.0014755332376807928,\n",
              "  0.0013885059161111712,\n",
              "  0.001306608784943819,\n",
              "  0.0012295421911403537,\n",
              "  0.0011570220813155174,\n",
              "  0.0010887804673984647,\n",
              "  0.0010245625162497163,\n",
              "  0.0009641298674978316,\n",
              "  0.0009072637185454369,\n",
              "  0.0008537503308616579,\n",
              "  0.0008033972699195147,\n",
              "  0.000756009598262608,\n",
              "  0.0007114199106581509,\n",
              "  0.000669458182528615,\n",
              "  0.0006299735978245735,\n",
              "  0.0005928152240812778,\n",
              "  0.0005578483687713742,\n",
              "  0.0005249465466476977,\n",
              "  0.0004939845530316234,\n",
              "  0.00046484952326864004,\n",
              "  0.00043743071728385985,\n",
              "  0.00041163034620694816,\n",
              "  0.00038735143607482314,\n",
              "  0.0003645053075160831,\n",
              "  0.0003430057258810848,\n",
              "  0.00032277576974593103,\n",
              "  0.0003037361311726272,\n",
              "  0.00028582257800735533,\n",
              "  0.00026896269991993904,\n",
              "  0.0002530993369873613,\n",
              "  0.00023817119654268026,\n",
              "  0.00022412321413867176,\n",
              "  0.0002109034830937162,\n",
              "  0.00019846452050842345,\n",
              "  0.00018675789760891348,\n",
              "  0.00017574333469383419,\n",
              "  0.00016537783085368574,\n",
              "  0.00015562406042590737,\n",
              "  0.00014644456678070128,\n",
              "  0.00013780697190668434,\n",
              "  0.0001296785776503384,\n",
              "  0.00012203054211568087,\n",
              "  0.0001148335068137385,\n",
              "  0.00010806007776409388,\n",
              "  0.00010168715380132198,\n",
              "  9.568879613652825e-05,\n",
              "  9.004558523884043e-05,\n",
              "  8.473347406834364e-05,\n",
              "  7.973633182700723e-05,\n",
              "  7.503328379243612e-05,\n",
              "  7.060711504891515e-05,\n",
              "  6.644259701715782e-05,\n",
              "  6.25242610112764e-05,\n",
              "  5.88366856391076e-05,\n",
              "  5.536639946512878e-05,\n",
              "  5.2100542234256864e-05,\n",
              "  4.9027494242182e-05,\n",
              "  4.6136043238220736e-05,\n",
              "  4.341443127486855e-05,\n",
              "  4.0853454265743494e-05,\n",
              "  3.8444217352662235e-05,\n",
              "  3.6176996218273416e-05,\n",
              "  3.4042721381410956e-05,\n",
              "  3.20353101415094e-05,\n",
              "  3.0145227356115356e-05,\n",
              "  2.8367545382934622e-05,\n",
              "  2.6694444386521354e-05,\n",
              "  2.512018909328617e-05,\n",
              "  2.3638542188564315e-05,\n",
              "  2.2244286810746416e-05,\n",
              "  2.0931995095452294e-05,\n",
              "  1.969770892173983e-05,\n",
              "  1.8535407434683293e-05,\n",
              "  1.7442132957512513e-05,\n",
              "  1.6413749108323827e-05,\n",
              "  1.544540464237798e-05,\n",
              "  1.453438017051667e-05,\n",
              "  1.367720960843144e-05,\n",
              "  1.2870462342107203e-05,\n",
              "  1.2111167961847968e-05,\n",
              "  1.1397018170100637e-05,\n",
              "  1.0724835192377213e-05,\n",
              "  1.0092169759445824e-05,\n",
              "  9.497090104559902e-06,\n",
              "  8.936822268879041e-06,\n",
              "  8.409861038671806e-06,\n",
              "  7.913886292953975e-06,\n",
              "  7.447044026775984e-06,\n",
              "  7.007870408415329e-06,\n",
              "  6.594548722205218e-06,\n",
              "  6.205558747751638e-06,\n",
              "  5.839513050887035e-06,\n",
              "  5.494944616657449e-06,\n",
              "  5.170919394004159e-06,\n",
              "  4.865952178079169e-06,\n",
              "  4.578946573019493e-06,\n",
              "  4.308852112444583e-06,\n",
              "  4.054825694765896e-06,\n",
              "  3.8154803405632265e-06,\n",
              "  3.590414280552068e-06,\n",
              "  3.378747578608454e-06,\n",
              "  3.179361556249205e-06,\n",
              "  2.991840119648259e-06,\n",
              "  2.815437710523838e-06,\n",
              "  2.6494685698708054e-06]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDQBglt64SrO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "5488a877-de25-4512-ce01-58c320bffbf2"
      },
      "source": [
        "# Plot training & validation loss values\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRBsXuB_V8pO",
        "colab_type": "text"
      },
      "source": [
        "> 학습이 끝났으니, 이번에는 새로운 데이터(샘플)에 대한 모델의 예측값을 확인해 볼까요? \n",
        "만일 입력데이터 5라는 새로운 샘플이 주어졌을때, 출력은 어떻게 될까요? \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rik3PJGV8x-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_predict = model.predict(np.array([5]))\n",
        "print(y_predict)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}