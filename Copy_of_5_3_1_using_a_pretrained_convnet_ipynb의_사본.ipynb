{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Copy of 5.3.1-using-a-pretrained-convnet.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sturu1/THU-PM/blob/master/Copy_of_5_3_1_using_a_pretrained_convnet_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xpUSdGdbjWn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwRyf6DCjWoD",
        "colab_type": "text"
      },
      "source": [
        "# 사전 훈련된 컨브넷 사용하기\n",
        "\n",
        "이 노트북은 [케라스 창시자에게 배우는 딥러닝](https://tensorflow.blog/%EC%BC%80%EB%9D%BC%EC%8A%A4-%EB%94%A5%EB%9F%AC%EB%8B%9D/) 책의 5장 3절의 코드 예제입니다. 책에는 더 많은 내용과 그림이 있습니다. 이 노트북에는 소스 코드에 관련된 설명만 포함합니다.\n",
        "\n",
        "----\n",
        "\n",
        "작은 이미지 데이터셋에 딥러닝을 적용하는 일반적이고 매우 효과적인 방법은 **사전 훈련된 네트워크를 사용**하는 것입니다. 사전 훈련된 네트워크는 일반적으로 대규모 이미지 분류 문제를 위해 **대량의 데이터셋에서 미리 훈련되어 저장된 네트워크**입니다. 원본 데이터셋이 충분히 크고 일반적이라면 사전 훈련된 네트워크에 의해 학습된 특성의 계층 구조는 실제 세상에 대한 일반적인 모델로 효율적인 역할을 할 수 있습니다. 새로운 문제가 원래 작업과 완전히 다른 클래스에 대한 것이더라도 이런 특성은 많은 컴퓨터 비전 문제에 유용합니다. 예를 들어 (대부분 동물이나 생활 용품으로 이루어진) ImageNet 데이터셋에 네트워크를 훈련합니다. 그다음 이 네트워크를 이미지에서 가구 아이템을 식별하는 것 같은 다른 용도로 사용할 수 있습니다. 학습된 특성을 다른 문제에 적용할 수 있는 이런 유연성은 이전의 많은 얕은 학습 방법과 비교했을 때 딥러닝의 핵심 장점입니다. 이런 방식으로 작은 데이터셋을 가진 문제에도 딥러닝이 효율적으로 작동할 수 있습니다.\n",
        "\n",
        "여기에서는 (1.4백만 개의 레이블된 이미지와 1,000개의 클래스로 이루어진) **ImageNet 데이터셋**에서 훈련된 대규모 컨브넷을 사용해 보겠습니다. ImageNet 데이터셋은 다양한 종의 강아지와 고양이를 포함해 많은 동물들을 포함하고 있습니다. 그래서 강아지 vs. 고양이 분류 문제에 좋은 성능을 낼 것 같습니다.\n",
        "\n",
        "캐런 시몬연과 앤드류 지서먼이 2014년에 개발한 **VGG16 구조를 사용**하겠습니다. VGG16은 간단하고 ImageNet 데이터셋에 널리 사용되는 컨브넷 구조입니다. VGG16은 조금 오래되었고 최고 수준의 성능에는 못미치며 최근의 다른 모델보다는 조금 무겁습니다. 하지만 이 모델의 구조가 이전에 보았던 것과 비슷해서 새로운 개념을 도입하지 않고 이해하기 쉽기 때문에 선택했습니다. 아마 VGG가 처음 보는 모델 애칭일지 모르겠습니다. 이런 이름에는 VGG, ResNet, Inception, Inception-ResNet, Xception 등이 있습니다. 컴퓨터 비전을 위해 딥러닝을 계속 공부하다보면 이런 이름을 자주 만나게 될 것입니다.\n",
        "\n",
        "사전 훈련된 네트워크를 사용하는 두 가지 방법이 있습니다. **(1)특성 추출과 (2)미세 조정**입니다. 이 두 가지를 모두 다루어 보겠습니다. 먼저 특성 추출부터 시작하죠."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hNkn8BpjWoE",
        "colab_type": "text"
      },
      "source": [
        "## 특성 추출\n",
        "\n",
        "특성 추출은 사전에 학습된 네트워크의 표현을 사용해 새로운 샘플에서 흥미로운 특성을 뽑아내는 것입니다. 이런 특성을 사용하여 새로운 분류기를 처음부터 훈련합니다.\n",
        "\n",
        "앞서 보았듯이 컨브넷은 이미지 분류를 위해 두 부분으로 구성됩니다. 먼저 연속된 합성곱과 풀링 층으로 시작해서 완전 연결 분류기로 끝납니다. 첫 번째 부분을 모델의 합성곱 기반층(convolutional base)이라고 부르겠습니다. 컨브넷의 경우 **특성 추출은 사전에 훈련된 네트워크의 합성곱 기반층을 선택해 새로운 데이터를 통과시키고 그 출력으로 새로운 분류기를 훈련합니다.**\n",
        "\n",
        "![swapping FC classifiers](https://s3.amazonaws.com/book.keras.io/img/ch5/swapping_fc_classifier.png)\n",
        "\n",
        "왜 합성곱 층만 재사용할까요? 완전 연결 분류기도 재사용할 수 있을까요? 일반적으로 권장하지 않습니다. 합성곱 층에 의해 학습된 표현이 더 일반적이어서 재사용 가능하기 때문입니다. 컨브넷의 특성 맵은 사진에 대한 **일반적인 컨셉의 존재 여부를 기록한 맵**입니다. 그래서 **주어진 컴퓨터 비전 문제에 상관없이 유용하게 사용할 수** 있습니다. 하지만 **분류기에서 학습한 표현은 모델이 훈련된 클래스 집합에 특화**되어 있습니다. 분류기는 전체 사진에 어떤 클래스가 존재할 확률에 관한 정보만을 담고 있습니다. 더군다나 완전 연결 층에서 찾은 표현은 더 이상 입력 이미지에 있는 객체의 위치 정보를 가지고 있지 않습니다. 완전 연결 층들은 공간 개념을 제거하지만 합성곱의 특성 맵은 객체의 위치를 고려합니다. 객체의 위치가 중요한 문제라면 완전 연결 층에서 만든 특성은 크게 쓸모가 없습니다.\n",
        "\n",
        "특정 합성곱 층에서 추출한 표현의 일반성(그리고 재사용성)의 수준은 모델에 있는 층의 깊이에 달려 있습니다. 모델의 하위 층은 (에지, 색깔, 질감 등과 같이) 지역적이고 매우 일반적인 특성 맵을 추출합니다. 반면 상위 층은 ('강아지 눈'이나 '고양이 귀'와 같이) 좀 더 추상적인 개념을 추출합니다. 만약 **새로운 데이터셋이 원본 모델이 훈련한 데이터셋과 많이 다르다면 전체 합성곱 기반층을 사용하는 것보다는 모델의 하위 층 몇 개만 특성 추출에 사용하는 것이 좋습니다.**\n",
        "\n",
        "ImageNet의 클래스 집합에는 여러 종류의 강아지와 고양이를 포함하고 있습니다. 이런 경우 원본 모델의 완전 연결 층에 있는 정보를 재사용하는 것이 도움이 될 것 같습니다. 하지만 새로운 문제의 클래스가 원본 모델의 클래스 집합과 겹치지 않는 좀 더 일반적인 경우를 다루기 위해서 여기서는 완전 연결 층을 사용하지 않겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4IloqVPjWoF",
        "colab_type": "text"
      },
      "source": [
        "ImageNet 데이터셋에 훈련된 VGG16 네트워크의 합성곱 기반층을 사용하여 강아지와 고양이 이미지에서 유용한 특성을 추출해 보겠습니다. 그런 다음 이 특성으로 강아지 vs. 고양이 분류기를 훈련합니다.\n",
        "\n",
        "VGG16 모델은 케라스에 패키지로 포함되어 있습니다. `keras.applications` 모듈에서 임포트할 수 있습니다. `keras.applications` 모듈에서 사용 가능한 이미지 분류 모델은 다음과 같습니다(모두 ImageNet 데이터셋에서 훈련되었습니다):\n",
        "\n",
        "* Xception\n",
        "* InceptionV3\n",
        "* ResNet50\n",
        "* VGG16\n",
        "* VGG19\n",
        "* MobileNet\n",
        "\n",
        "VGG16 모델을 만들어 보죠:\n",
        "\n",
        "먼저 앞의 예제에서 사용한 코드를 이용해 훈련 데이터를 colab에 구성하겠습니다. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2NFNB_ZQ1Mn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "84e5b095-b69a-49e9-9b55-ef31756925f6"
      },
      "source": [
        "!git clone https://github.com/gilbutITbook/006975.git\n",
        "\n",
        "import os, shutil\n",
        "original_dataset_dir = '/content/006975/datasets/cats_and_dogs/train'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '006975'...\n",
            "remote: Enumerating objects: 102534, done.\u001b[K\n",
            "remote: Total 102534 (delta 0), reused 0 (delta 0), pack-reused 102534\u001b[K\n",
            "Receiving objects: 100% (102534/102534), 202.75 MiB | 14.97 MiB/s, done.\n",
            "Resolving deltas: 100% (11/11), done.\n",
            "Checking out files: 100% (104042/104042), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55BDhZuLzMsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 소규모 데이터셋을 저장할 디렉터리\n",
        "base_dir = 'cats_and_dogs_small'\n",
        "if os.path.exists(base_dir):  # 반복적인 실행을 위해 디렉토리를 삭제합니다.\n",
        "    shutil.rmtree(base_dir)   # 이 코드는 책에 포함되어 있지 않습니다.\n",
        "os.mkdir(base_dir)\n",
        "# 훈련, 검증, 테스트 분할을 위한 디렉터리\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "os.mkdir(train_dir)\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "os.mkdir(validation_dir)\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "os.mkdir(test_dir)\n",
        "# 훈련용 고양이 사진 디렉터리\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "os.mkdir(train_cats_dir)\n",
        "# 훈련용 강아지 사진 디렉터리\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "os.mkdir(train_dogs_dir)\n",
        "# 검증용 고양이 사진 디렉터리\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "os.mkdir(validation_cats_dir)\n",
        "# 검증용 강아지 사진 디렉터리\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
        "os.mkdir(validation_dogs_dir)\n",
        "# 테스트용 고양이 사진 디렉터리\n",
        "test_cats_dir = os.path.join(test_dir, 'cats')\n",
        "os.mkdir(test_cats_dir)\n",
        "# 테스트용 강아지 사진 디렉터리\n",
        "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
        "os.mkdir(test_dogs_dir)\n",
        "# 처음 1,000개의 고양이 이미지를 train_cats_dir에 복사합니다\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(train_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "# 다음 500개 고양이 이미지를 validation_cats_dir에 복사합니다\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(validation_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "# 다음 500개 고양이 이미지를 test_cats_dir에 복사합니다\n",
        "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(test_cats_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "# 처음 1,000개의 강아지 이미지를 train_dogs_dir에 복사합니다\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(train_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "# 다음 500개 강아지 이미지를 validation_dogs_dir에 복사합니다\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(validation_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)\n",
        "    \n",
        "# 다음 500개 강아지 이미지를 test_dogs_dir에 복사합니다\n",
        "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
        "for fname in fnames:\n",
        "    src = os.path.join(original_dataset_dir, fname)\n",
        "    dst = os.path.join(test_dogs_dir, fname)\n",
        "    shutil.copyfile(src, dst)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nli9c9MCRKJK",
        "colab_type": "text"
      },
      "source": [
        "VGG16 모델을 불러와서 사용해 보겠습니다. \n",
        "\n",
        "[참고] 다음과 같이 실행하면 keras에 내장되어 있는 사전훈련된 네트워크들의 목록을 확인할 수 있습니다. \n",
        "\n",
        "`from keras import applications`\n",
        "\n",
        "`dir(applications)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_x4tsKjjWoG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import VGG16"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evTIlqSObCXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3ceada75-211d-414d-f431-447e754248ad"
      },
      "source": [
        "conv_base = VGG16(weights = 'imagenet', include_top = False, input_shape = (150, 150, 3))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztaWknCjbfGs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        },
        "outputId": "98808097-2f4f-47bd-92a8-ef7a970ba4b6"
      },
      "source": [
        "conv_base.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 150, 150, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wg_u6ABvjWoL",
        "colab_type": "text"
      },
      "source": [
        "VGG16 함수에 세 개의 매개변수를 전달합니다:\n",
        "\n",
        "* `weights`는 모델을 초기화할 가중치 체크포인트를 지정합니다.\n",
        "* `include_top`은 네트워크의 최상위 완전 연결 분류기를 포함할지 안할지를 지정합니다. 기본값은 ImageNet의 1,000개의 클래스에 대응되는 완전 연결 분류기를 포함합니다. 별도의 (강아지와 고양이 두 개의 클래스를 구분하는) 완전 연결 층을 추가하려고 하므로 이를 포함시키지 않습니다.\n",
        "* `input_shape`은 네트워크에 주입할 이미지 텐서의 크기입니다. 이 매개변수는 선택사항입니다. 이 값을 지정하지 않으면 네트워크가 어떤 크기의 입력도 처리할 수 있습니다.\n",
        "\n",
        "다음은 VGG16 합성곱 기반층의 자세한 구조입니다. 이 구조는 앞에서 보았던 간단한 컨브넷과 비슷합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHAqiUD5jWoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqP1hYRecWSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datagen = ImageDataGenerator(rescale = 1./255)\n",
        "batch_size = 20"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HnbHmOyci15",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_features(directory, sample_count):\n",
        "  features = np.zeros(shape = (sample_count, 4, 4, 512))\n",
        "  labels = np.zeros(shape = (sample_count))\n",
        "  generator = datagen.flow_from_directory(\n",
        "      directory, target_size = (150, 150), batch_size = batch_size, class_mode = 'binary')\n",
        "  i = 0\n",
        "  for inputs_batch, labels_batch in generator:\n",
        "    features_batch = conv_base.predict(inputs_batch)\n",
        "    features[i * batch_size : (i + 1) * batch_size] = features_batch\n",
        "    labels[i * batch_size : (i + 1) * batch_size] = labels_batch\n",
        "    i += 1\n",
        "    if i * batch_size >= sample_count:\n",
        "      break\n",
        "\n",
        "  return features, labels"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnTSVhmffsb7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f4154ff1-90d3-4c60-ff99-721c72f03bbf"
      },
      "source": [
        "train_features, train_labels = extract_features(train_dir, 2000)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miZTgSQEgWnh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cb8a9712-0882-459d-cff2-dfb219ae0e23"
      },
      "source": [
        "validation_features, validation_labels = extract_features(validation_dir, 1000)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FRh0bMlgWal",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "017c979d-834e-49ea-abff-d7869fe2043f"
      },
      "source": [
        "test_features, test_labels = extract_features(test_dir, 1000)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rGO1tRvgcGg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cac24bc2-7889-4f11-ee36-4f21ba379a5a"
      },
      "source": [
        "train_features.shape, train_labels.shape"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2000, 8192), (2000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gy4XJgazg_aA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features = np.reshape(train_features, (2000, 4*4*512))\n",
        "validation_features = np.reshape(validation_features, (1000, 4*4*512))\n",
        "test_features = np.reshape(test_features, (1000, 4*4*512))"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkkLtuDEhfkZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import optimizers"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eZKnPRxhZLC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2647300d-0409-4d36-c424-d44e6c987101"
      },
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Dense(256, activation = 'relu', input_dim = 4 * 4 * 512))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
        "model.compile(optimizer = optimizers.RMSprop(lr=2e-5), loss = 'binary_crossentropy', metrics = ['acc'])\n",
        "history = model.fit(train_features, train_labels, epochs = 30, batch_size = 20,\n",
        "                    validation_data = (validation_features, validation_labels))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 2000 samples, validate on 1000 samples\n",
            "Epoch 1/30\n",
            "2000/2000 [==============================] - 1s 506us/step - loss: 0.6050 - acc: 0.6670 - val_loss: 0.4355 - val_acc: 0.8510\n",
            "Epoch 2/30\n",
            "2000/2000 [==============================] - 1s 377us/step - loss: 0.4209 - acc: 0.8115 - val_loss: 0.3548 - val_acc: 0.8650\n",
            "Epoch 3/30\n",
            "2000/2000 [==============================] - 1s 382us/step - loss: 0.3464 - acc: 0.8590 - val_loss: 0.3198 - val_acc: 0.8780\n",
            "Epoch 4/30\n",
            "2000/2000 [==============================] - 1s 374us/step - loss: 0.3148 - acc: 0.8710 - val_loss: 0.2981 - val_acc: 0.8850\n",
            "Epoch 5/30\n",
            "2000/2000 [==============================] - 1s 382us/step - loss: 0.2819 - acc: 0.8900 - val_loss: 0.2805 - val_acc: 0.8860\n",
            "Epoch 6/30\n",
            "2000/2000 [==============================] - 1s 391us/step - loss: 0.2654 - acc: 0.8950 - val_loss: 0.2834 - val_acc: 0.8810\n",
            "Epoch 7/30\n",
            "2000/2000 [==============================] - 1s 389us/step - loss: 0.2464 - acc: 0.9020 - val_loss: 0.2641 - val_acc: 0.8960\n",
            "Epoch 8/30\n",
            "2000/2000 [==============================] - 1s 379us/step - loss: 0.2303 - acc: 0.9170 - val_loss: 0.2605 - val_acc: 0.8950\n",
            "Epoch 9/30\n",
            "2000/2000 [==============================] - 1s 381us/step - loss: 0.2174 - acc: 0.9185 - val_loss: 0.2576 - val_acc: 0.9030\n",
            "Epoch 10/30\n",
            "2000/2000 [==============================] - 1s 373us/step - loss: 0.2063 - acc: 0.9205 - val_loss: 0.2522 - val_acc: 0.9000\n",
            "Epoch 11/30\n",
            "2000/2000 [==============================] - 1s 382us/step - loss: 0.1949 - acc: 0.9320 - val_loss: 0.2448 - val_acc: 0.9000\n",
            "Epoch 12/30\n",
            "2000/2000 [==============================] - 1s 381us/step - loss: 0.1941 - acc: 0.9300 - val_loss: 0.2450 - val_acc: 0.9050\n",
            "Epoch 13/30\n",
            "2000/2000 [==============================] - 1s 383us/step - loss: 0.1796 - acc: 0.9400 - val_loss: 0.2413 - val_acc: 0.9040\n",
            "Epoch 14/30\n",
            "2000/2000 [==============================] - 1s 392us/step - loss: 0.1703 - acc: 0.9415 - val_loss: 0.2436 - val_acc: 0.9030\n",
            "Epoch 15/30\n",
            "2000/2000 [==============================] - 1s 383us/step - loss: 0.1627 - acc: 0.9430 - val_loss: 0.2402 - val_acc: 0.9060\n",
            "Epoch 16/30\n",
            "2000/2000 [==============================] - 1s 383us/step - loss: 0.1486 - acc: 0.9535 - val_loss: 0.2496 - val_acc: 0.8970\n",
            "Epoch 17/30\n",
            "2000/2000 [==============================] - 1s 374us/step - loss: 0.1478 - acc: 0.9505 - val_loss: 0.2364 - val_acc: 0.9080\n",
            "Epoch 18/30\n",
            "2000/2000 [==============================] - 1s 383us/step - loss: 0.1471 - acc: 0.9500 - val_loss: 0.2409 - val_acc: 0.8990\n",
            "Epoch 19/30\n",
            "2000/2000 [==============================] - 1s 388us/step - loss: 0.1438 - acc: 0.9510 - val_loss: 0.2361 - val_acc: 0.9050\n",
            "Epoch 20/30\n",
            "2000/2000 [==============================] - 1s 384us/step - loss: 0.1335 - acc: 0.9575 - val_loss: 0.2402 - val_acc: 0.9030\n",
            "Epoch 21/30\n",
            "2000/2000 [==============================] - 1s 378us/step - loss: 0.1314 - acc: 0.9540 - val_loss: 0.2406 - val_acc: 0.9050\n",
            "Epoch 22/30\n",
            "2000/2000 [==============================] - 1s 407us/step - loss: 0.1225 - acc: 0.9575 - val_loss: 0.2380 - val_acc: 0.9040\n",
            "Epoch 23/30\n",
            "2000/2000 [==============================] - 1s 395us/step - loss: 0.1186 - acc: 0.9635 - val_loss: 0.2351 - val_acc: 0.9070\n",
            "Epoch 24/30\n",
            "2000/2000 [==============================] - 1s 402us/step - loss: 0.1159 - acc: 0.9600 - val_loss: 0.2349 - val_acc: 0.9070\n",
            "Epoch 25/30\n",
            "2000/2000 [==============================] - 1s 411us/step - loss: 0.1067 - acc: 0.9605 - val_loss: 0.2385 - val_acc: 0.9020\n",
            "Epoch 26/30\n",
            "2000/2000 [==============================] - 1s 395us/step - loss: 0.1039 - acc: 0.9680 - val_loss: 0.2393 - val_acc: 0.9070\n",
            "Epoch 27/30\n",
            "2000/2000 [==============================] - 1s 386us/step - loss: 0.0972 - acc: 0.9690 - val_loss: 0.2353 - val_acc: 0.9060\n",
            "Epoch 28/30\n",
            "2000/2000 [==============================] - 1s 385us/step - loss: 0.0928 - acc: 0.9710 - val_loss: 0.2429 - val_acc: 0.9050\n",
            "Epoch 29/30\n",
            "2000/2000 [==============================] - 1s 386us/step - loss: 0.0900 - acc: 0.9725 - val_loss: 0.2382 - val_acc: 0.9060\n",
            "Epoch 30/30\n",
            "2000/2000 [==============================] - 1s 381us/step - loss: 0.0892 - acc: 0.9770 - val_loss: 0.2380 - val_acc: 0.9080\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1HDkyv8iqnD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "27b5bdb3-ab10-45ce-e3fb-5c1d4e789125"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo')\n",
        "plt.plot(epochs, val_acc, 'b')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No handles with labels found to put in legend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSKTavFQje4r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f9f3bd2-8be6-4d78-878a-a67cf327b580"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_features, test_labels)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 108us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ2ea0X4jnwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b6fa19e-bba8-4100-cb4a-cb6256a5d897"
      },
      "source": [
        "test_acc"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8880000114440918"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJzfidDPjWoV",
        "colab_type": "text"
      },
      "source": [
        "최종 특성 맵의 크기는 `(4, 4, 512)`입니다. 이 특성 위에 완전 연결 층을 놓을 것입니다.\n",
        "이 지점에서 두 가지 방식이 가능합니다.\n",
        "\n",
        "* 새로운 데이터셋에서 합성곱 기반층을 실행하고 출력을 넘파이 배열로 디스크에 저장합니다. 그다음 이 데이터를 이 책의 1부에서 보았던 것과 비슷한 독립된 완전 연결 분류기에 입력으로 사용합니다. 합성곱 연산은 전체 과정 중에서 가장 비싼 부분입니다. 이 방식은 모든 입력 이미지에 대해 합성곱 기반층을 한 번만 실행하면 되기 때문에 빠르고 비용이 적게 듭니다. 하지만 이런 이유 때문에 이 기법에는 데이터 증식을 사용할 수 없습니다.\n",
        "* 준비한 모델(`conv_base`) 위에 `Dense` 층을 쌓아 확장합니다. 그다음 입력 데이터에서 엔드 투 엔드로 전체 모델을 실행합니다. 모델에 노출된 모든 입력 이미지가 매번 합성곱 기반층을 통과하기 때문에 데이터 증식을 사용할 수 있습니다. 하지만 이런 이유로 이 방식은 첫 번째 방식보다 훨씬 비용이 많이 듭니다.\n",
        "\n",
        "두 가지 방식을 모두 다루어 보겠습니다. 첫 번째 방식을 구현하는 코드를 살펴봅니다. `conv_base`에 데이터를 주입하고 출력을 기록합니다. 이 출력을 새로운 모델의 입력으로 사용하겠습니다.\n",
        "\n",
        "먼저 앞서 소개한 `ImageDataGenerator`를 사용해 이미지와 레이블을 넘파이 배열로 추출하겠습니다. `conv_base` 모델의 `predict` 메서드를 호출하여 이 이미지에서 특성을 추출합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6hpo8FtjWoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-SlW6lNjWob",
        "colab_type": "text"
      },
      "source": [
        "추출된 특성의 크기는 `(samples, 4, 4, 512)`입니다. 완전 연결 분류기에 주입하기 위해서 먼저 `(samples, 8192)` 크기로 펼칩니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dF84E7-8jWoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQP1rQQzjWoi",
        "colab_type": "text"
      },
      "source": [
        "그러고 나서 완전 연결 분류기를 정의하고(규제를 위해 드롭아웃을 사용합니다) 저장된 데이터와 레이블을 사용해 훈련합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2s528Z8jWoj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Til_1oFvjWoo",
        "colab_type": "text"
      },
      "source": [
        "두 개의 `Dense` 층만 처리하면 되기 때문에 훈련이 매우 빠릅니다. CPU를 사용하더라도 한 에포크에 걸리는 시간이 1초 미만입니다.\n",
        "\n",
        "훈련 손실과 정확도 곡선을 살펴보죠:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9_zM-NFjWoo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MbbiqS4jWou",
        "colab_type": "text"
      },
      "source": [
        "약 90%의 검증 정확도에 도달했습니다. 이전 절에서 처음부터 훈련시킨 작은 모델에서 얻은 것보다 훨씬 좋습니다. 하지만 이 그래프는 많은 비율로 드롭아웃을 사용했음에도 불구하고 훈련이 시작하면서 거의 바로 과대적합되고 있다는 것을 보여줍니다. 작은 이미지 데이터셋에서는 과대적합을 막기 위해 필수적인 데이터 증식을 사용하지 않았기 때문입니다.\n"
      ]
    }
  ]
}